---
{"dg-publish":true,"permalink":"/research/reading-notes/bender-climbing-nlu-meaning2020/","title":"Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data","tags":["literature-note","gardenEntry","gardenEntry","gardenEntry","gardenEntry","gardenEntry"]}
---



# [Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data](zotero://select/library/items/NJWGGMR2)

> [!citation]+
>**Bibliography:** Bender, Emily M., and Alexander Koller. “Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data.” In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, 5185–98. Online: Association for Computational Linguistics, 2020. [https://doi.org/10.18653/v1/2020.acl-main.463](https://doi.org/10.18653/v1/2020.acl-main.463).
>
>**Page-no:** 5185
>
>
>
>**PDF:** [Bender and Koller - 2020 - Climbing towards NLU On Meaning, Form, and Unders.pdf](file:///C:\Users\Henry%20Imler\Zotero\storage\XZ3PL57K\Bender%20and%20Koller%20-%202020%20-%20Climbing%20towards%20NLU%20On%20Meaning,%20Form,%20and%20Unders.pdf)
>
>[**Open in Zotero**](zotero://select/library/items/NJWGGMR2)
>[**Open DOI**](https://doi.org/10.18653/v1/2020.acl-main.463)

> [!abstract]+
> 
> The success of the large neural language models on many NLP tasks is exciting. However, we ﬁnd that these successes sometimes lead to hype in which these models are being described as “understanding” language or capturing “meaning”. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of “Taking Stock of Where We’ve Been and Where We’re Going”, we argue that a clear understanding of the distinction between form and meaning will help guide the ﬁeld towards better science around natural language understanding.
> 
 
## Tags
- 

## Annotations
>[!Annotation|#ffd400]+ 
>the language modeling task, because it only uses form as training data, cannot in principle lead to learning of meaning. (p [1](zotero://open-pdf/library/items/XZ3PL57K?page=1&annotation=Q65DM24W).)

>[!Annotation|#ffd400]+ 
>language model to refer to any system trained only on the task of string prediction, whether it operates over characters, words or sentences, and sequentially or not. (p [1](zotero://open-pdf/library/items/XZ3PL57K?page=1&annotation=Z6ENAHID).)

>[!Annotation|#ffd400]+ 
>(linguistic) meaning to be the relation between a linguistic form and communicative intent. (p [1](zotero://open-pdf/library/items/XZ3PL57K?page=1&annotation=EAAPRRUF).)

>[!Annotation|#5fb236]+ 
>potential as unsupervised open-domain QA systems. (p [2](zotero://open-pdf/library/items/XZ3PL57K?page=2&annotation=ZU9W3LVB).) 
>
>No to mention as perpetual and unsupervised extraction machines.

>[!Annotation|#2ea8e5]+ 
>One important consequence of imprudent use of terminology in our academic discourse is that it feeds AI hype in the popular press. (p [2](zotero://open-pdf/library/items/XZ3PL57K?page=2&annotation=A7WGYJTA).)

>[!Annotation|#e56eee]+ 
>In some cases, NLP experts speaking with the media are being appropriately careful, as in these two quotes in the New York Times:1 (4) These systems are still a really long way from truly understanding running prose. (Gary Marcus) (5) Though BERT passed the lab’s common-sense test, machines are still a long way from an artificial version of a human’s common sense. (Oren Etzioni) (p [2](zotero://open-pdf/library/items/XZ3PL57K?page=2&annotation=4VSYA2FP).)

>[!Annotation|#5fb236]+ 
>Part of the reason for this tendency to use imprecise language may well be that we do not yet fully understand what exactly it is about language that the large LMs come to implicitly represent. (p [2](zotero://open-pdf/library/items/XZ3PL57K?page=2&annotation=L4339VXP).)

>[!Annotation|#ffd400]+ 
>We take form to be any observable realization of language: marks (p [2](zotero://open-pdf/library/items/XZ3PL57K?page=2&annotation=4RABSJ2E).)

>[!Annotation|#ffd400]+ 
>on a page, pixels or bytes in a digital representation of text, or movements of the articulators.5 (p [3](zotero://open-pdf/library/items/XZ3PL57K?page=3&annotation=ZHGGHDFN).)

>[!Annotation|#ffd400]+ 
>We take meaning to be the relation between the form and something external to language, (p [3](zotero://open-pdf/library/items/XZ3PL57K?page=3&annotation=QUM4FJF7).)

>[!Annotation|#ffd400]+ 
>active participation of the listener is crucial to human communication (Reddy, 1979; Clark, 1996). (p [3](zotero://open-pdf/library/items/XZ3PL57K?page=3&annotation=3WBUHZ98).)

>[!Annotation|#ff6666]+ 
>We humans are also very willing, as we will see in §4 below, to attribute communicative intent to a linguistic signal of a language we speak, even if the originator of the signal is not an entity that could have communicative intent. (p [3](zotero://open-pdf/library/items/XZ3PL57K?page=3&annotation=FBII7Y9G).)

>[!Annotation|#ffd400]+ 
>Meaning and understanding have long been seen as key to intelligence. Turing (1950) argued that a machine can be said to “think” if a human judge cannot distinguish it from a human interlocutor after having an arbitrary written conversation with (p [3](zotero://open-pdf/library/items/XZ3PL57K?page=3&annotation=FJPP8VDR).)

>[!Annotation|#ffd400]+ 
>each. However, humans are quick to attribute meaning and even intelligence to artificial agents, even when they know them to be artificial, as evidenced by the way people formed attachments to ELIZA (Weizenbaum, 1966; Block, 1981). (p [4](zotero://open-pdf/library/items/XZ3PL57K?page=4&annotation=METQ3IUG).)

>[!Annotation|#a28ae5]+ 
>This means we must be extra careful in devising evaluations for machine understanding, as Searle (1980) elaborates with his Chinese Room experiment: he develops the metaphor of a “system” in which a person who does not speak Chinese answers Chinese questions by consulting a library of Chinese books according to predefined rules. From the outside, the system seems like it “understands” Chinese, although in reality no actual understanding happens anywhere inside the system. (p [4](zotero://open-pdf/library/items/XZ3PL57K?page=4&annotation=SCXWLPIU).)

>[!Annotation|#ffd400]+ 
>it is possible to manipulate forms well enough to be indistinguishable from a system that understands the meaning of the forms, reasons about it, and responds appropriately. (p [4](zotero://open-pdf/library/items/XZ3PL57K?page=4&annotation=MXANJ82G).)

>[!Annotation|#f19837]+ 
>4 The octopus test In order to illustrate the challenges in attempting to learn meaning from form alone, we propose a concrete scenario. Say that A and B, both fluent speakers of English, are independently stranded on two uninhabited islands. They soon discover that previous visitors to these islands have left behind telegraphs and that they can communicate with each other via an underwater cable. A and B start happily typing messages to each other. Meanwhile, O, a hyper-intelligent deep-sea octopus who is unable to visit or observe the two islands, discovers a way to tap into the underwater cable and listen in on A and B’s conversations. O knows nothing about English initially, but is very good at detecting statistical patterns. Over time, O learns to predict with great accuracy how B will respond to each of A’s utterances. O also observes that certain words tend to occur in similar contexts, and perhaps learns to generalize across lexical patterns by hypothesizing that they can be used somewhat interchangeably. Nonetheless, O has never observed these objects, and thus would not be able to pick out the referent of a word when presented with a set of (physical) alternatives. At some point, O starts feeling lonely. He cuts the underwater cable and inserts himself into the conversation, by pretending to be B and replying to A’s messages.
>
Can O successfully pose as B without making A suspicious?
>
This constitutes a weak form of the Turing test (weak because A has no reason to suspect she is talking to a nonhuman); the interesting question is whether O fails it because he has not learned the meaning relation, having seen only the form of A and B’s utterances. The extent to which O can fool A depends on the task — that is, on what A is trying to talk about. A and B have spent a lot of time exchanging trivial notes about their daily lives to make the long island evenings more enjoyable. It seems possible that O would be able to produce new sentences of the kind B used to produce; essentially acting as a chatbot. This is because the utterances in such conversations have a primarily social function, and do not need to be grounded in the particulars of the interlocutors’ actual physical situation nor anything else specific about the real world. It is sufficient to produce text that is internally coherent. Now say that A has invented a new device, say a coconut catapult. She excitedly sends detailed instructions on building a coconut catapult to B, and asks about B’s experiences and suggestions for improvements. Even if O had a way of constructing the catapult underwater, he does not know what words such as rope and coconut refer to, and thus can’t physically reproduce the experiment. He can only resort to earlier observations about how B responded to similarly worded utterances. Perhaps O can recognize utterances about mangos and nails as “similarly worded” because those words appeared in similar contexts as coconut and rope. So O decides to simply say “Cool idea, great job!”, because B said that a lot when A talked about ropes and nails. It is absolutely conceivable that A accepts this reply as meaningful — but only because A does all the work in attributing meaning to O’s response. It is not because O understood the meaning of A’s instructions or even his own reply. Finally, A faces an emergency. She is suddenly pursued by an angry bear. She grabs a couple of sticks and frantically asks B to come up with a way to construct a weapon to defend herself. Of course, O has no idea what A “means”. Solving a task like this requires the ability to map accurately between words and real-world entities (as well as reasoning and creative thinking). It is at this point that O would fail the Turing test, if A hadn’t been eaten by the bear before noticing the deception.7 Having only form available as training data, O did not learn meaning. The language exchanged by A and B is a projection of their communicative intents through the meaning relation into linguistic forms. Without access to a means of hypothesizing and testing the underlying communicative intents, reconstructing them from the forms alone is hopeless, and O’s language use will eventually diverge from the language use of an agent who can ground their language in coherent communicative intents. The thought experiment also illustrates our point from §3 about listeners’ active role in communication. When O sent signals to A pretending to be B, he exploited statistical regularities in the form, i.e. the distribution of linguistic forms he observed. Whatever O learned is a reflection of A and B’s communicative intents and the meaning relation. But reproducing this distribution is not sufficient for meaningful communication. O only fooled A into believing he was B because A was such an active listener: Because agents who produce English sentences usually have communicative intents, she 7To see what a large LM might reply in this situation, we prompted the GPT-2 demo with “Help! I’m being chased by a bear! All I have is these sticks. What should I do?”, and GPT2 to supplied “You’re not going to get away with this!” (https://gpt2.apps.allenai.org/, accessed 2019/12/4). Following Radford et al.’s (2019) approach of giving explicit cues to encode the task, we also constructed a more elaborate prompt. The results, given in Appendix A, are highly entertaining but no more helpful to the hapless A. assumes that O does too, and thus she builds the conventional meaning English associates with O’s utterances. Because she assumes that O is B, she uses that conventional meaning together with her other guesses about B’s state of mind and goals to attribute communicative intent. It is not that O’s utterances make sense, but rather, that A can make sense of them. (p [5](zotero://open-pdf/library/items/XZ3PL57K?page=5&annotation=D7S4UE32).)


